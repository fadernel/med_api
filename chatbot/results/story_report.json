{
  "action_listen": {
    "precision": 1.0,
    "recall": 0.84375,
    "f1-score": 0.9152542372881356,
    "support": 32
  },
  "utter_requeriment": {
    "precision": 0.4,
    "recall": 1.0,
    "f1-score": 0.5714285714285715,
    "support": 2
  },
  "reco_pneumonia": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "what_effusion": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "affirm": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "mood_great": {
    "precision": 0.75,
    "recall": 0.75,
    "f1-score": 0.75,
    "support": 4
  },
  "utter_give_what_effusion": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_happy": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 5
  },
  "what_nodule": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "mood_unhappy": {
    "precision": 1.0,
    "recall": 0.3333333333333333,
    "f1-score": 0.5,
    "support": 3
  },
  "show_pdf": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_cheer_up": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3
  },
  "utter_give_recomendations_pneumothorax": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "deny": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_give_recomendations_pneumonia": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_give_treat_mass": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_greet": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 8
  },
  "nlu_fallback": {
    "precision": 0.16666666666666666,
    "recall": 1.0,
    "f1-score": 0.2857142857142857,
    "support": 1
  },
  "reco_infiltration": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_did_that_help": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3
  },
  "reco_pneumothorax": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_give_recomendations_infiltration": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_goodbye": {
    "precision": 1.0,
    "recall": 0.5,
    "f1-score": 0.6666666666666666,
    "support": 4
  },
  "goodbye": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_pdf_xray": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "utter_please_rephrase": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_iamabot": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "greet": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 8
  },
  "use_model": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_give_what_atelectasis": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "bot_challenge": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "user_form": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "what_atelectasis": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "treat_mass": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_give_what_nodule": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_fill_data": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "micro avg": {
    "precision": 0.9052631578947369,
    "recall": 0.8349514563106796,
    "f1-score": 0.8686868686868687,
    "support": 103
  },
  "macro avg": {
    "precision": 0.7587962962962963,
    "recall": 0.7618634259259259,
    "f1-score": 0.7413628822527127,
    "support": 103
  },
  "weighted avg": {
    "precision": 0.902588996763754,
    "recall": 0.8349514563106796,
    "f1-score": 0.8532366377520236,
    "support": 103
  },
  "accuracy": 0.8349514563106796,
  "conversation_accuracy": {
    "accuracy": 0.3157894736842105,
    "correct": 6,
    "with_warnings": 0,
    "total": 19
  }
}